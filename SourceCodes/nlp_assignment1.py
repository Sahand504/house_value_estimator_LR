# -*- coding: utf-8 -*-
"""NLP_Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kfYc9DnzyX8-RUIHKzgLwy3AnCBdbpJz
"""

from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

dataset = pd.read_csv("/content/drive/My Drive/NLP/housing.csv")
dataset = dataset.dropna()
print("Here are the first 10 rows of the dataset :)")
dataset.head(10)

longitude = dataset['longitude']
latitude = dataset["latitude"]
housing_median_age = dataset["housing_median_age"]
total_rooms = dataset["total_rooms"]
total_bedrooms = dataset["total_bedrooms"]
population = dataset["population"]
households = dataset["households"]
median_income = dataset["median_income"]
median_house_value = dataset["median_house_value"]

fig, ax = plt.subplots(9, 1, figsize=(10, 20))
for i in range(9):
  ax[i].xaxis.set_ticks(np.arange(0, 18, 1))
ax[0].plot(longitude[0:18])
ax[0].set_ylabel("longitude")
ax[1].plot(latitude[0:18], color="red")
ax[1].set_ylabel("latitude")
ax[2].plot(housing_median_age[0:18])
ax[2].set_ylabel("housing median age")
ax[3].plot(total_rooms[0:18], color="red")
ax[3].set_ylabel("total rooms")
ax[4].plot(total_bedrooms[0:18])
ax[4].set_ylabel("total bedrooms")
ax[5].plot(population[0:18], color="red")
ax[5].set_ylabel("population")
ax[6].plot(households[0:18])
ax[6].set_ylabel("households")
ax[7].plot(median_income[0:18], color="red")
ax[7].set_ylabel("median income")
ax[8].plot(median_house_value[0:18])
ax[8].set_xlabel("sample number")
ax[8].set_ylabel("median house value")
plt.show()

X = dataset.loc[:, "longitude": "median_income"]
Y = dataset["median_house_value"]

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

x_train_np = x_train.to_numpy()
x_test_np = x_test.to_numpy()
y_train_np = y_train.to_numpy()
y_test_np = y_test.to_numpy()

# from sklearn.feature_selection import SelectKBest
# from sklearn.feature_selection import chi2

# min_latitude = x_train["latitude"].min()
# min_longitude = x_train["longitude"].min()
# x_train["latitude"] = x_train["latitude"] + abs(min_latitude)
# x_train["longitude"] = x_train["longitude"] + abs(min_longitude)

# #apply SelectKBest class to extract top 8 best features
# bestfeatures = SelectKBest(score_func=chi2, k=8)
# fit = bestfeatures.fit(x_train,y_train)
# dfscores = pd.DataFrame(fit.scores_)
# dfcolumns = pd.DataFrame(x_train.columns)
# #concat two dataframes for better visualization 
# featureScores = pd.concat([dfcolumns,dfscores],axis=1)
# featureScores.columns = ['Specs','Score']  #naming the dataframe columns

# print(featureScores.nlargest(5,'Score'))  #print 5 best features

# # feature selection
# # delete unimportant features

# del x_train["latitude"]
# del x_train["longitude"]
# del x_train["median_income"]
# del x_train["housing_median_age"]

# del x_test["latitude"]
# del x_test["longitude"]
# del x_test["median_income"]
# del x_test["housing_median_age"]

model = linear_model.LinearRegression()
model.fit(x_train, y_train)
predictions = model.predict(x_test)

mse = mean_squared_error(y_test, predictions)
print("Model's MSE is: " + str(mse))

r2 = r2_score(y_test, predictions)
print("Model's R^2 is: " + str(r2))

import torch
from torch.nn import Conv1d
from torch.nn import MaxPool1d
from torch.nn import Flatten
from torch.nn import Linear
from torch.nn.functional import relu
from torch.utils.data import DataLoader, TensorDataset

class CnnRegressor(torch.nn.Module):
  def __init__(self, batch_size, inputs, outputs):
    super(CnnRegressor, self).__init__()
    self.batch_size = batch_size
    self.inputs = inputs
    self.outputs = outputs

    self.input_layer = Conv1d(inputs, batch_size, 1)
    self.max_pooling_layer = MaxPool1d(1)
    self.conv_layer = Conv1d(batch_size, 256, kernel_size=4, stride=2, padding=2)
    self.flatten_layer = Flatten()
    self.linear_layer = Linear(256, 64)
    self.output_layer = Linear(64, outputs)

  def feed(self, input):
    input = input.reshape((self.batch_size, self.inputs, 1))
    output = relu(self.input_layer(input))
    output = self.max_pooling_layer(output)
    output = relu(self.conv_layer(output))
    output = self.flatten_layer(output)
    output = self.linear_layer(output)
    output = self.output_layer(output)
    return output

from torch.optim import SGD
from torch.nn import L1Loss

!pip install pytorch-ignite
from ignite.contrib.metrics.regression.r2_score import R2Score

batch_size = 256
model = CnnRegressor(batch_size, X.shape[1], 1)
model.cuda()

def model_loss(model, dataset, train=False, optimizer=None):
  performance = L1Loss()
  score_metric = R2Score()

  avg_loss = 0
  avg_score = 0
  count = 0
  for input, output in iter(dataset):
    predictions = model.feed(input)
    loss = performance(predictions, output)
    score_metric.update([predictions, output])
    score = score_metric.compute()
    if(train):
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      
    avg_loss += loss.item()
    avg_score += score
    count += 1

    return avg_loss / count, avg_score / count

epochs = 10000
lr = 1e-5
optimizer = SGD(model.parameters(), lr=lr)
inputs = torch.from_numpy(x_train_np).cuda().float()
outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0], 1)).cuda().float()
tensor = TensorDataset(inputs, outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)

losses = []
r2_scores = []
for cnt, epoch in enumerate(range(epochs)):
  avg_loss, avg_r2_score = model_loss(model, loader, train=True, optimizer=optimizer)
  losses.append(avg_loss)
  r2_scores.append(avg_r2_score)
  if(cnt % 2000 == 1):
    lr /= 2
  if(cnt % 100 == 1):
    print("Epoch " + str(epoch + 1) + ":\n\tLoss = " + str(avg_loss) + "\n\tR^2 Score = " + str(avg_r2_score))

fig, ax = plt.subplots(2, 1, figsize=(10, 20))
ax[0].plot(losses, ".")
ax[1].plot(r2_scores, "o")

inputs = torch.from_numpy(x_test_np).cuda().float()
outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0], 1)).cuda().float()

tensor = TensorDataset(inputs, outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)
avg_loss, avg_r2_score = model_loss(model, loader)
print("The model's L1 loss is: " + str(avg_loss))
print("The model's R^2 score is: " + str(avg_r2_score))

